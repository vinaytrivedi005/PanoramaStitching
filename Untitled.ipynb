{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "\n",
    "### code is copied from https://www.pyimagesearch.com/2016/01/11/opencv-panorama-stitching/\n",
    "from PIL import Image, ImageChops\n",
    "import numpy as np\n",
    "import imutils\n",
    "import cv2\n",
    "from matplotlib import cm\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stitcher:\n",
    "    def __init__(self):\n",
    "        # determine if we are using OpenCV v3.X\n",
    "        self.isv3 = imutils.is_cv3()\n",
    "    \n",
    "    # Original method to stitch 2 images horizontal or vertical\n",
    "    \n",
    "    \"\"\"def stitch(self, imageB, imageA, ratio=0.75, reprojThresh=4.0,\n",
    "        showMatches=False, stitchDirection='HORIZONTAL'):\n",
    "        # unpack the images, then detect keypoints and extract\n",
    "        # local invariant descriptors from them\n",
    "        #(imageB, imageA) = images\n",
    "        (kpsA, featuresA) = self.detectAndDescribe(imageA)\n",
    "        (kpsB, featuresB) = self.detectAndDescribe(imageB)\n",
    " \n",
    "        # match features between the two images\n",
    "        M = self.matchKeypoints(kpsA, kpsB,\n",
    "            featuresA, featuresB, ratio, reprojThresh)\n",
    " \n",
    "        # if the match is None, then there aren't enough matched\n",
    "        # keypoints to create a panorama\n",
    "        if M is None:\n",
    "            return None\n",
    "        # otherwise, apply a perspective warp to stitch the images\n",
    "        # together\n",
    "        (matches, H, status) = M\n",
    "        result = ''\n",
    "        if stitchDirection=='HORIZONTAL':\n",
    "            result = cv2.warpPerspective(imageA, H, (imageA.shape[1] + imageB.shape[1], imageA.shape[0]))\n",
    "            result[0:imageB.shape[0], 0:imageB.shape[1]] = imageB\n",
    "        else:\n",
    "            result = cv2.warpPerspective(imageA, H, (imageA.shape[1], imageA.shape[0] + imageB.shape[0]))\n",
    "            result[0:imageB.shape[0], 0:imageB.shape[1]] = imageB\n",
    " \n",
    "        # check to see if the keypoint matches should be visualized\n",
    "        if showMatches:\n",
    "            vis = self.drawMatches(imageA, imageB, kpsA, kpsB, matches, status)\n",
    " \n",
    "            # return a tuple of the stitched image and the\n",
    "            # visualization\n",
    "            return (result, vis)\n",
    " \n",
    "        # return the stitched image\n",
    "        return result\"\"\"\n",
    "    \n",
    "    ##################################\n",
    "    \n",
    "    def stitchHorizontal(self, images, ratio=0.75, reprojThresh=4.0, showMatches=False, trim_threshold=50):\n",
    "        return self.stitch(images, ratio, reprojThresh, showMatches,trim_threshold= trim_threshold, stitchDirection='HORIZONTAL')\n",
    "        \n",
    "    def stitchVertical(self, images, ratio=0.75, reprojThresh=4.0, showMatches=False, trim_threshold=50):\n",
    "        return self.stitch(images, ratio, reprojThresh, showMatches, trim_threshold= trim_threshold, stitchDirection='VERTICAL')\n",
    "    \n",
    "    def stitch(self, images, ratio=0.75, reprojThresh=4.0,\n",
    "        showMatches=False, stitchDirection='HORIZONTAL', trim_threshold=50):\n",
    "        # unpack the images, then detect keypoints and extract\n",
    "        # local invariant descriptors from them\n",
    "        result = None\n",
    "        for j in range(0, len(images)-1):\n",
    "            \n",
    "            if result is None:\n",
    "                result = images[j]\n",
    "            #(imageB, imageA) = imutils.resize(result,height=(images[i][j+1]).shape[1]), images[i][j+1]\n",
    "            (imageB, imageA) = result, images[j+1]\n",
    "            #print(imageA.shape)\n",
    "            (kpsA, featuresA) = self.detectAndDescribe(imageA)\n",
    "            (kpsB, featuresB) = self.detectAndDescribe(imageB)\n",
    "\n",
    "            # match features between the two images\n",
    "            M = self.matchKeypoints(kpsA, kpsB,\n",
    "                featuresA, featuresB, ratio, reprojThresh)\n",
    "\n",
    "            # if the match is None, then there aren't enough matched\n",
    "            # keypoints to create a panorama\n",
    "            if M is None:\n",
    "                return None\n",
    "            # otherwise, apply a perspective warp to stitch the images\n",
    "            # together\n",
    "            (matches, H, status) = M\n",
    "            if stitchDirection=='HORIZONTAL':\n",
    "                result = cv2.warpPerspective(imageA, H, (imageA.shape[1] + imageB.shape[1], imageA.shape[0]))\n",
    "                result[0:imageB.shape[0], 0:imageB.shape[1]] = imageB\n",
    "                result = self.trim_image(result, trim_threshold)\n",
    "            else:\n",
    "                result = cv2.warpPerspective(imageA, H, (imageA.shape[1], imageA.shape[0] + imageB.shape[0]))\n",
    "                result[0:imageB.shape[0], 0:imageB.shape[1]] = imageB\n",
    "                result = self.trim_image(result, trim_threshold= trim_threshold, trimFrom='BOTTOM')\n",
    "\n",
    "            \n",
    "            \n",
    "                \n",
    " \n",
    "        # check to see if the keypoint matches should be visualized\n",
    "        if showMatches:\n",
    "            vis = self.drawMatches(imageA, imageB, kpsA, kpsB, matches, status)\n",
    "            \n",
    " \n",
    "            # return a tuple of the stitched image and the\n",
    "            # visualization\n",
    "            return (result, vis)\n",
    " \n",
    "        # return the stitched image\n",
    "        return result\n",
    "    \n",
    "    ##################################\n",
    "    def detectAndDescribe(self, image):\n",
    "        # convert the image to grayscale\n",
    "        #gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    " \n",
    "        # check to see if we are using OpenCV 3.X\n",
    "        if self.isv3:\n",
    "            # detect and extract features from the image\n",
    "            descriptor = cv2.xfeatures2d.SIFT_create()\n",
    "            (kps, features) = descriptor.detectAndCompute(image, None)\n",
    " \n",
    "        # otherwise, we are using OpenCV 2.4.X\n",
    "        else:\n",
    "            # detect keypoints in the image\n",
    "            detector = cv2.FeatureDetector_create(\"SIFT\")\n",
    "            kps = detector.detect(gray)\n",
    " \n",
    "            # extract features from the image\n",
    "            extractor = cv2.DescriptorExtractor_create(\"SIFT\")\n",
    "            (kps, features) = extractor.compute(gray, kps)\n",
    " \n",
    "        # convert the keypoints from KeyPoint objects to NumPy\n",
    "        # arrays\n",
    "        kps = np.float32([kp.pt for kp in kps])\n",
    " \n",
    "        # return a tuple of keypoints and features\n",
    "        return (kps, features)\n",
    "    \n",
    "    def matchKeypoints(self, kpsA, kpsB, featuresA, featuresB,\n",
    "        ratio, reprojThresh):\n",
    "        # compute the raw matches and initialize the list of actual\n",
    "        # matches\n",
    "        matcher = cv2.DescriptorMatcher_create(\"BruteForce\")\n",
    "        rawMatches = matcher.knnMatch(featuresA, featuresB, 2)\n",
    "        matches = []\n",
    " \n",
    "        # loop over the raw matches\n",
    "        for m in rawMatches:\n",
    "            # ensure the distance is within a certain ratio of each\n",
    "            # other (i.e. Lowe's ratio test)\n",
    "            if len(m) == 2 and m[0].distance < m[1].distance * ratio:\n",
    "                matches.append((m[0].trainIdx, m[0].queryIdx))\n",
    "                \n",
    "        # computing a homography requires at least 4 matches\n",
    "        if len(matches) > 4:\n",
    "            # construct the two sets of points\n",
    "            ptsA = np.float32([kpsA[i] for (_, i) in matches])\n",
    "            ptsB = np.float32([kpsB[i] for (i, _) in matches])\n",
    " \n",
    "            # compute the homography between the two sets of points\n",
    "            (H, status) = cv2.findHomography(ptsA, ptsB, cv2.RANSAC,\n",
    "                reprojThresh)\n",
    " \n",
    "            # return the matches along with the homograpy matrix\n",
    "            # and status of each matched point\n",
    "            return (matches, H, status)\n",
    " \n",
    "        # otherwise, no homograpy could be computed\n",
    "        return None\n",
    "    \n",
    "    def drawMatches(self, imageA, imageB, kpsA, kpsB, matches, status):\n",
    "        # initialize the output visualization image\n",
    "        (hA, wA) = imageA.shape[:2]\n",
    "        (hB, wB) = imageB.shape[:2]\n",
    "        vis = np.zeros((max(hA, hB), wA + wB, 3), dtype=\"uint8\")\n",
    "        vis[0:hA, 0:wA] = imageA\n",
    "        vis[0:hB, wA:] = imageB\n",
    " \n",
    "        # loop over the matches\n",
    "        for ((trainIdx, queryIdx), s) in zip(matches, status):\n",
    "            # only process the match if the keypoint was successfully\n",
    "            # matched\n",
    "            if s == 1:\n",
    "                # draw the match\n",
    "                ptA = (int(kpsA[queryIdx][0]), int(kpsA[queryIdx][1]))\n",
    "                ptB = (int(kpsB[trainIdx][0]) + wA, int(kpsB[trainIdx][1]))\n",
    "                cv2.line(vis, ptA, ptB, (0, 255, 0), 1)\n",
    " \n",
    "        # return the visualization\n",
    "        return vis\n",
    "    \n",
    "    def trim_image(self, image, trimFrom='RIGHT', trim_threshold=100):\n",
    "        x=0\n",
    "        k=-1\n",
    "        if trimFrom=='RIGHT':\n",
    "            while x ==0:\n",
    "                if k==(-1)*image.shape[1]:\n",
    "                    print(\"no zeroes found\")\n",
    "                    break\n",
    "                if np.count_nonzero(image[:,k]==(0,0,0))>=trim_threshold:\n",
    "                    k=k-1\n",
    "                else:    \n",
    "                    image=image[:,0:k,:]\n",
    "                    #print(k)\n",
    "                    break\n",
    "            return image\n",
    "        else:\n",
    "            while x ==0:\n",
    "                if k==(-1)*image.shape[0]:\n",
    "                    break\n",
    "                if np.count_nonzero(image[k,:]==(0,0,0))>=trim_threshold:\n",
    "                    k=k-1\n",
    "                else:    \n",
    "                    image=image[0:k,:,:]\n",
    "                    #print(k)\n",
    "                    break\n",
    "            return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the two images and resize them to have a width of 400 pixels\n",
    "# (for faster processing)\n",
    "\n",
    "\n",
    "\n",
    "imageA = cv2.imread('images/rack2_image1.jpg')\n",
    "imageB = cv2.imread('images/rack2_image2.jpg')\n",
    "imageC = cv2.imread('images/rack2_image3.jpg')\n",
    "imageD = cv2.imread('images/rack2_image4.jpg')\n",
    "imageE = cv2.imread('images/rack2_image5.jpg')\n",
    "imageF = cv2.imread('images/rack2_image6.jpg')\n",
    "imageG = cv2.imread('images/rack2_image7.jpg')\n",
    "imageH = cv2.imread('images/rack2_image8.jpg')\n",
    "imageI = cv2.imread('images/rack_image9.jpg')\n",
    "imageJ = cv2.imread('images/rack_image10.jpg')\n",
    "imageK = cv2.imread('images/rack_image11.jpg')\n",
    "imageL = cv2.imread('images/rack_image12.jpg')\n",
    "imageM = cv2.imread('images/rack_image13.jpg')\n",
    "\n",
    "#result0 = cv2.imread(\"images/rack_result0.jpg\")\n",
    "#imageA = imutils.resize(imageA, width=400)\n",
    "#imageB = imutils.resize(imageB, width=400)\n",
    "#imageC = imutils.resize(imageC, width=400)\n",
    "#imageD = imutils.resize(imageD, width=400)\n",
    "\n",
    "#cv2.imshow(\"Image A\", imageA)\n",
    "#cv2.imshow(\"Image B\", imageB)\n",
    "#cv2.imshow(\"Image C\", imageC)\n",
    "#cv2.imshow(\"Image D\", imageD)\n",
    "#cv2.imshow(\"1\", images[0])\n",
    "#[imageM,imageL,imageK,imageJ,imageI,imageH,imageG,imageF,imageE,imageD,imageC,imageB,imageA]\n",
    "#images = [imageA, imageB, imageC, imageD, imageE, imageF, imageG, imageH]\n",
    "images = [imageA, imageC, imageE]   \n",
    "    # stitch the images together to create a panorama\n",
    "stitcher = Stitcher()\n",
    "result = None\n",
    "for i in range(0, len(images)-1):\n",
    "    if i==0:\n",
    "        result = images[i]\n",
    "\n",
    "    (result, vis) = stitcher.stitchHorizontal([result, images[i+1]], ratio=0.5, reprojThresh=20.0, showMatches=True)\n",
    "#result = imutils.resize(result, height=imageD.shape[1])\n",
    "#cv2.imwrite(\"images/rack2_result0.jpg\",result)\n",
    "#cv2.imwrite(\"images/output.jpg\",result) \n",
    "# show the images\n",
    "\n",
    "    cv2.imshow(\"Keypoint Matches\"+str(i), vis)\n",
    "    cv2.imshow(\"Result\"+str(i), result)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the two images and resize them to have a width of 400 pixels\n",
    "# (for faster processing)\n",
    "image1 = 'images/img_hpart1.jpg'\n",
    "image2 = 'images/img_hpart2.jpg'\n",
    "image3 = 'images/img_hpart3.jpg'\n",
    "image4 = 'images/img_hpart4.jpg'\n",
    "imageA = cv2.imread(image1)\n",
    "imageB = cv2.imread(image2)\n",
    "imageC = cv2.imread(image3)\n",
    "imageD = cv2.imread(image4)\n",
    "\n",
    "#imageA = imutils.resize(imageA, width=400)\n",
    "#imageB = imutils.resize(imageB, width=400)\n",
    "#imageC = imutils.resize(imageC, width=400)\n",
    "#imageD = imutils.resize(imageD, width=400)\n",
    "\n",
    "#cv2.imshow(\"Image A\", imageA)\n",
    "#cv2.imshow(\"Image B\", imageB)\n",
    "#cv2.imshow(\"Image C\", imageC)\n",
    "#cv2.imshow(\"Image D\", imageD)\n",
    "\n",
    "\n",
    "# stitch the images together to create a panorama\n",
    "stitcher = Stitcher()\n",
    "(result, vis) = stitcher.stitchHorizontal([imageA, imageB, imageC, imageD], showMatches=True)\n",
    "result = imutils.resize(result, height=imageD.shape[1])\n",
    "#cv2.imwrite(\"images/output.jpg\",result) \n",
    "# show the images\n",
    "\n",
    "#cv2.imshow(\"Keypoint Matches\", vis)\n",
    "cv2.imshow(\"Result\", result)\n",
    "\n",
    "################# VERTICAL TEST\n",
    "\n",
    "\"\"\"image1 = 'images/img_vpart1.jpg'\n",
    "image2 = 'images/img_vpart2.jpg'\n",
    "image3 = 'images/img_vpart3.jpg'\n",
    "image4 = 'images/img_vpart4.jpg'\n",
    "imageA = cv2.imread(image1)\n",
    "imageB = cv2.imread(image2)\n",
    "imageC = cv2.imread(image3)\n",
    "imageD = cv2.imread(image4)\n",
    "\n",
    "#imageA = imutils.resize(imageA, width=400)\n",
    "#imageB = imutils.resize(imageB, width=400)\n",
    "#imageC = imutils.resize(imageC, width=400)\n",
    "#imageD = imutils.resize(imageD, width=400)\n",
    "\n",
    "#cv2.imshow(\"Image A\", imageA)\n",
    "#cv2.imshow(\"Image B\", imageB)\n",
    "#cv2.imshow(\"Image C\", imageC)\n",
    "#cv2.imshow(\"Image D\", imageD)\n",
    "\n",
    "\n",
    "# stitch the images together to create a panorama\n",
    "stitcher = Stitcher()\n",
    "(result, vis) = stitcher.stitchVertical([imageA, imageB, imageC, imageD], showMatches=True)\n",
    "result = imutils.resize(result, width=imageD.shape[0])\n",
    "#cv2.imwrite(\"images/output.jpg\",result) \n",
    "# show the images\n",
    "\n",
    "#cv2.imshow(\"Keypoint Matches\", vis)\n",
    "cv2.imshow(\"Result\", result)\"\"\"\n",
    "\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyallwindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from time import sleep\n",
    "\n",
    "cap = cv2.VideoCapture(\"videos/rack_video1.mp4\")\n",
    "stitcher = Stitcher()\n",
    "result = None\n",
    "counter = 0\n",
    "while True:\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        sleep(5)\n",
    "    ret, image = cap.read()\n",
    "    if ret:\n",
    "        cv2.imshow(\"frame #\"+str(counter), image)\n",
    "        if counter%5 == 0:\n",
    "            #image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            (result, vis) = stitcher.stitchHorizontal([result, image], showMatches=True)\n",
    "            result = imutils.resize(result, height=image.shape[1])\n",
    "        counter+=1\n",
    "    else:\n",
    "        print(\"cannot read video\")\n",
    "        break\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stitcher = Stitcher()\n",
    "result = None\n",
    "counter = 0\n",
    "count=1\n",
    "def process_image(image):\n",
    "    \n",
    "    global counter\n",
    "    global count\n",
    "    global result\n",
    "    global stitcher\n",
    "    #plt.imshow(image)\n",
    "    #cv2.imshow(\"abc\", image)\n",
    "    if counter == 0:\n",
    "        result = image\n",
    "        cv2.imwrite(\"images/rack4_image\"+str(count)+\".jpg\",image)\n",
    "    elif counter%20 == 0:\n",
    "        #image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        #zeroes = np.zeros((15*count,image.shape[1],3), dtype=np.uint8)\n",
    "        #temp_image = cv2.resize(image, (image.shape[1], image.shape[0]-9*count) )\n",
    "        #image = np.concatenate((temp_image, zeroes), axis=0)\n",
    "        (result, vis) = stitcher.stitchHorizontal([result, image], showMatches=True, trim_threshold=40)\n",
    "        \n",
    "        #result = imutils.resize(result, height=image.shape[1])\n",
    "        cv2.imwrite(\"result/rack4_result\"+str(count)+\".jpg\",result)\n",
    "        cv2.imwrite(\"images/rack4_image\"+str(count)+\".jpg\",image)\n",
    "        count+=1\n",
    "    counter+=1\n",
    "    return imutils.resize(result, height=image.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video videos/rack4_video4_output.mp4\n",
      "[MoviePy] Writing video videos/rack4_video4_output.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 299/353 [00:36<00:06,  8.21it/s]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-175>\u001b[0m in \u001b[0;36mwrite_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, progress_bar)\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/default/lib/python3.6/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36mrequires_duration\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Attribute 'duration' not set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-174>\u001b[0m in \u001b[0;36mwrite_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, progress_bar)\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/default/lib/python3.6/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36muse_clip_fps_by_default\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m    135\u001b[0m              for (k,v) in k.items()}\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mnew_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<decorator-gen-173>\u001b[0m in \u001b[0;36mwrite_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, progress_bar)\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/default/lib/python3.6/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36mconvert_masks_to_RGB\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_RGB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/default/lib/python3.6/site-packages/moviepy/video/VideoClip.py\u001b[0m in \u001b[0;36mwrite_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, progress_bar)\u001b[0m\n\u001b[1;32m    325\u001b[0m                            \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                            \u001b[0mffmpeg_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mffmpeg_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                            progress_bar=progress_bar)\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mremove_temp\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmake_audio\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/default/lib/python3.6/site-packages/moviepy/video/io/ffmpeg_writer.py\u001b[0m in \u001b[0;36mffmpeg_write_video\u001b[0;34m(clip, filename, fps, codec, bitrate, preset, withmask, write_logfile, audiofile, verbose, threads, ffmpeg_params, progress_bar)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         for t,frame in clip.iter_frames(progress_bar=progress_bar, with_times=True,\n\u001b[0;32m--> 218\u001b[0;31m                                         fps=fps, dtype=\"uint8\"):\n\u001b[0m\u001b[1;32m    219\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwithmask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/default/lib/python3.6/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m \"\"\", fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/default/lib/python3.6/site-packages/moviepy/Clip.py\u001b[0m in \u001b[0;36mgenerator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m                 \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-134>\u001b[0m in \u001b[0;36mget_frame\u001b[0;34m(self, t)\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/default/lib/python3.6/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(f, *a, **kw)\u001b[0m\n\u001b[1;32m     87\u001b[0m         new_kw = {k: fun(v) if k in varnames else v\n\u001b[1;32m     88\u001b[0m                  for (k,v) in kw.items()}\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/default/lib/python3.6/site-packages/moviepy/Clip.py\u001b[0m in \u001b[0;36mget_frame\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_duration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/default/lib/python3.6/site-packages/moviepy/Clip.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;31m#mf = copy(self.make_frame)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mnewclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_make_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkeep_duration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/default/lib/python3.6/site-packages/moviepy/video/VideoClip.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(gf, t)\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mapply_to\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mapply_to\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mgf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimage_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;31m# --------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-52768a5bf234>\u001b[0m in \u001b[0;36mprocess_image\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m#temp_image = cv2.resize(image, (image.shape[1], image.shape[0]-9*count) )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#image = np.concatenate((temp_image, zeroes), axis=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvis\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstitcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstitchHorizontal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshowMatches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m#result = imutils.resize(result, height=image.shape[1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "white_output = 'videos/rack4_video4_output.mp4'\n",
    "## To speed up the testing process you may want to try your pipeline on a shorter subclip of the video\n",
    "## To do so add .subclip(start_second,end_second) to the end of the line below\n",
    "## Where start_second and end_second are integer values representing the start and end of the subclip\n",
    "## You may also uncomment the following line for a subclip of the first 5 seconds\n",
    "clip1 = VideoFileClip(\"videos/rack4_video4.mp4\")\n",
    "#clip1 = VideoFileClip(\"test_videos/solidWhiteRight.mp4\")\n",
    "white_clip = clip1.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "%time white_clip.write_videofile(white_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 8\n",
    "for i in range(7,0,-1):\n",
    "    image = cv2.imread(\"images/rack4_image\"+str(i)+\".jpg\")\n",
    "    if i==7:\n",
    "        result = image\n",
    "    else:\n",
    "        (result, vis) = stitcher.stitchHorizontal([image, result], showMatches=True)\n",
    "        cv2.imwrite(\"images/rack4_result\"+str(count)+\".jpg\",result)\n",
    "        count+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageA = cv2.imread(\"images/rack4_result7.jpg\")\n",
    "imageB = cv2.imread(\"images/rack4_result13.jpg\")\n",
    "(result, vis) = stitcher.stitchHorizontal([imageA, imageB], showMatches=True)\n",
    "cv2.imwrite(\"images/rack4_result14.jpg\",result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(imageB.shape)\n",
    "plt.imshow(imageB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"images/rack4_image1.jpg\")\n",
    "print(img.shape)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"images/rack4_image1.jpg\")\n",
    "print(image.shape)\n",
    "zeroes = np.zeros((9,image.shape[1],3), dtype=int)\n",
    "print(zeroes.shape)\n",
    "print(zeroes)\n",
    "temp_image = cv2.resize(image, (image.shape[1], image.shape[0]-9) )\n",
    "print(temp_image.shape)\n",
    "print(temp_image)\n",
    "#plt.imshow(temp_image)\n",
    "img = np.concatenate((temp_image, zeroes), axis=0)\n",
    "#zeroes[0:image.shape[0]-9,:] = temp_image[:,:]\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the two images and resize them to have a width of 400 pixels\n",
    "# (for faster processing)\n",
    "\n",
    "\n",
    "\n",
    "imageA = cv2.imread('images/rack_image1.jpg')\n",
    "imageB = cv2.imread('images/rack_image2.jpg')\n",
    "imageC = cv2.imread('images/rack_image3.jpg')\n",
    "imageD = cv2.imread('images/rack_image4.jpg')\n",
    "imageE = cv2.imread('images/rack_image5.jpg')\n",
    "imageF = cv2.imread('images/rack_image6.jpg')\n",
    "imageG = cv2.imread('images/rack_image7.jpg')\n",
    "imageH = cv2.imread('images/rack_image8.jpg')\n",
    "imageI = cv2.imread('images/rack_image9.jpg')\n",
    "imageJ = cv2.imread('images/rack_image10.jpg')\n",
    "imageK = cv2.imread('images/rack_image11.jpg')\n",
    "imageL = cv2.imread('images/rack_image12.jpg')\n",
    "imageM = cv2.imread('images/rack_image13.jpg')\n",
    "\n",
    "#imageA = imutils.resize(imageA, width=400)\n",
    "#imageB = imutils.resize(imageB, width=400)\n",
    "#imageC = imutils.resize(imageC, width=400)\n",
    "#imageD = imutils.resize(imageD, width=400)\n",
    "\n",
    "#cv2.imshow(\"Image A\", imageA)\n",
    "#cv2.imshow(\"Image B\", imageB)\n",
    "#cv2.imshow(\"Image C\", imageC)\n",
    "#cv2.imshow(\"Image D\", imageD)\n",
    "#cv2.imshow(\"1\", images[0])\n",
    "#[imageM,imageL,imageK,imageJ,imageI,imageH,imageG,imageF,imageE,imageD,imageC,imageB,imageA]\n",
    "    \n",
    "    # stitch the images together to create a panorama\n",
    "stitcher = Stitcher()\n",
    "(result, vis) = stitcher.stitchHorizontal([imageD,imageA], showMatches=True)\n",
    "result = imutils.resize(result, height=imageD.shape[1])\n",
    "cv2.imwrite(\"images/rack_result0.jpg\",result)\n",
    "#cv2.imwrite(\"images/output.jpg\",result) \n",
    "# show the images\n",
    "\n",
    "#cv2.imshow(\"Keypoint Matches\", vis)\n",
    "cv2.imshow(\"Result\", result)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:default]",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
