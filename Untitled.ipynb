{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "\n",
    "### code is copied from https://www.pyimagesearch.com/2016/01/11/opencv-panorama-stitching/\n",
    "from PIL import Image, ImageChops\n",
    "import numpy as np\n",
    "import imutils\n",
    "import cv2\n",
    "from matplotlib import cm\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stitcher:\n",
    "    def __init__(self):\n",
    "        # determine if we are using OpenCV v3.X\n",
    "        self.isv3 = imutils.is_cv3()\n",
    "    \n",
    "    # Original method to stitch 2 images horizontal or vertical\n",
    "    \n",
    "    \"\"\"def stitch(self, imageB, imageA, ratio=0.75, reprojThresh=4.0,\n",
    "        showMatches=False, stitchDirection='HORIZONTAL'):\n",
    "        # unpack the images, then detect keypoints and extract\n",
    "        # local invariant descriptors from them\n",
    "        #(imageB, imageA) = images\n",
    "        (kpsA, featuresA) = self.detectAndDescribe(imageA)\n",
    "        (kpsB, featuresB) = self.detectAndDescribe(imageB)\n",
    " \n",
    "        # match features between the two images\n",
    "        M = self.matchKeypoints(kpsA, kpsB,\n",
    "            featuresA, featuresB, ratio, reprojThresh)\n",
    " \n",
    "        # if the match is None, then there aren't enough matched\n",
    "        # keypoints to create a panorama\n",
    "        if M is None:\n",
    "            return None\n",
    "        # otherwise, apply a perspective warp to stitch the images\n",
    "        # together\n",
    "        (matches, H, status) = M\n",
    "        result = ''\n",
    "        if stitchDirection=='HORIZONTAL':\n",
    "            result = cv2.warpPerspective(imageA, H, (imageA.shape[1] + imageB.shape[1], imageA.shape[0]))\n",
    "            result[0:imageB.shape[0], 0:imageB.shape[1]] = imageB\n",
    "        else:\n",
    "            result = cv2.warpPerspective(imageA, H, (imageA.shape[1], imageA.shape[0] + imageB.shape[0]))\n",
    "            result[0:imageB.shape[0], 0:imageB.shape[1]] = imageB\n",
    " \n",
    "        # check to see if the keypoint matches should be visualized\n",
    "        if showMatches:\n",
    "            vis = self.drawMatches(imageA, imageB, kpsA, kpsB, matches, status)\n",
    " \n",
    "            # return a tuple of the stitched image and the\n",
    "            # visualization\n",
    "            return (result, vis)\n",
    " \n",
    "        # return the stitched image\n",
    "        return result\"\"\"\n",
    "    \n",
    "    ##################################\n",
    "    \n",
    "    def stitchHorizontal(self, images, ratio=0.75, reprojThresh=4.0, showMatches=False, trim_threshold=50):\n",
    "        return self.stitch(images, ratio, reprojThresh, showMatches,trim_threshold= trim_threshold, stitchDirection='HORIZONTAL')\n",
    "        \n",
    "    def stitchVertical(self, images, ratio=0.75, reprojThresh=4.0, showMatches=False, trim_threshold=50):\n",
    "        return self.stitch(images, ratio, reprojThresh, showMatches, trim_threshold= trim_threshold, stitchDirection='VERTICAL')\n",
    "    \n",
    "    def stitch(self, images, ratio=0.75, reprojThresh=4.0,\n",
    "        showMatches=False, stitchDirection='HORIZONTAL', trim_threshold=50):\n",
    "        # unpack the images, then detect keypoints and extract\n",
    "        # local invariant descriptors from them\n",
    "        result = None\n",
    "        for j in range(0, len(images)-1):\n",
    "            \n",
    "            if result is None:\n",
    "                result = images[j]\n",
    "            #(imageB, imageA) = imutils.resize(result,height=(images[i][j+1]).shape[1]), images[i][j+1]\n",
    "            (imageB, imageA) = result, images[j+1]\n",
    "            #print(imageA.shape)\n",
    "            (kpsA, featuresA) = self.detectAndDescribe(imageA)\n",
    "            (kpsB, featuresB) = self.detectAndDescribe(imageB)\n",
    "\n",
    "            # match features between the two images\n",
    "            M = self.matchKeypoints(kpsA, kpsB,\n",
    "                featuresA, featuresB, ratio, reprojThresh)\n",
    "\n",
    "            # if the match is None, then there aren't enough matched\n",
    "            # keypoints to create a panorama\n",
    "            if M is None:\n",
    "                return None\n",
    "            # otherwise, apply a perspective warp to stitch the images\n",
    "            # together\n",
    "            (matches, H, status) = M\n",
    "            if stitchDirection=='HORIZONTAL':\n",
    "                result = cv2.warpPerspective(imageA, H, (imageA.shape[1] + imageB.shape[1], imageA.shape[0]))\n",
    "                result[0:imageB.shape[0], 0:imageB.shape[1]] = imageB\n",
    "                result = self.trim_image(result, trimFrom='RIGHT', trim_threshold=trim_threshold)\n",
    "            else:\n",
    "                result = cv2.warpPerspective(imageA, H, (imageA.shape[1], imageA.shape[0] + imageB.shape[0]))\n",
    "                result[0:imageB.shape[0], 0:imageB.shape[1]] = imageB\n",
    "                result = self.trim_image(result, trim_threshold= trim_threshold, trimFrom='BOTTOM')\n",
    "\n",
    "            \n",
    "            \n",
    "                \n",
    " \n",
    "        # check to see if the keypoint matches should be visualized\n",
    "        if showMatches:\n",
    "            vis = self.drawMatches(imageA, imageB, kpsA, kpsB, matches, status)\n",
    "            \n",
    " \n",
    "            # return a tuple of the stitched image and the\n",
    "            # visualization\n",
    "            return (result, vis)\n",
    " \n",
    "        # return the stitched image\n",
    "        return result\n",
    "    \n",
    "    ##################################\n",
    "    def detectAndDescribe(self, image):\n",
    "        # convert the image to grayscale\n",
    "        #gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    " \n",
    "        # check to see if we are using OpenCV 3.X\n",
    "        if self.isv3:\n",
    "            # detect and extract features from the image\n",
    "            descriptor = cv2.xfeatures2d.SIFT_create()\n",
    "            (kps, features) = descriptor.detectAndCompute(image, None)\n",
    " \n",
    "        # otherwise, we are using OpenCV 2.4.X\n",
    "        else:\n",
    "            # detect keypoints in the image\n",
    "            detector = cv2.FeatureDetector_create(\"SIFT\")\n",
    "            kps = detector.detect(gray)\n",
    " \n",
    "            # extract features from the image\n",
    "            extractor = cv2.DescriptorExtractor_create(\"SIFT\")\n",
    "            (kps, features) = extractor.compute(gray, kps)\n",
    " \n",
    "        # convert the keypoints from KeyPoint objects to NumPy\n",
    "        # arrays\n",
    "        kps = np.float32([kp.pt for kp in kps])\n",
    " \n",
    "        # return a tuple of keypoints and features\n",
    "        return (kps, features)\n",
    "    \n",
    "    def matchKeypoints(self, kpsA, kpsB, featuresA, featuresB,\n",
    "        ratio, reprojThresh):\n",
    "        # compute the raw matches and initialize the list of actual\n",
    "        # matches\n",
    "        matcher = cv2.DescriptorMatcher_create(\"BruteForce\")\n",
    "        rawMatches = matcher.knnMatch(featuresA, featuresB, 2)\n",
    "        matches = []\n",
    " \n",
    "        # loop over the raw matches\n",
    "        for m in rawMatches:\n",
    "            # ensure the distance is within a certain ratio of each\n",
    "            # other (i.e. Lowe's ratio test)\n",
    "            if len(m) == 2 and m[0].distance < m[1].distance * ratio:\n",
    "                matches.append((m[0].trainIdx, m[0].queryIdx))\n",
    "                \n",
    "        # computing a homography requires at least 4 matches\n",
    "        if len(matches) > 4:\n",
    "            # construct the two sets of points\n",
    "            ptsA = np.float32([kpsA[i] for (_, i) in matches])\n",
    "            ptsB = np.float32([kpsB[i] for (i, _) in matches])\n",
    " \n",
    "            # compute the homography between the two sets of points\n",
    "            (H, status) = cv2.findHomography(ptsA, ptsB, cv2.RANSAC,\n",
    "                reprojThresh)\n",
    " \n",
    "            # return the matches along with the homograpy matrix\n",
    "            # and status of each matched point\n",
    "            return (matches, H, status)\n",
    " \n",
    "        # otherwise, no homograpy could be computed\n",
    "        return None\n",
    "    \n",
    "    def drawMatches(self, imageA, imageB, kpsA, kpsB, matches, status):\n",
    "        # initialize the output visualization image\n",
    "        (hA, wA) = imageA.shape[:2]\n",
    "        (hB, wB) = imageB.shape[:2]\n",
    "        vis = np.zeros((max(hA, hB), wA + wB, 3), dtype=\"uint8\")\n",
    "        vis[0:hA, 0:wA] = imageA\n",
    "        vis[0:hB, wA:] = imageB\n",
    " \n",
    "        # loop over the matches\n",
    "        for ((trainIdx, queryIdx), s) in zip(matches, status):\n",
    "            # only process the match if the keypoint was successfully\n",
    "            # matched\n",
    "            if s == 1:\n",
    "                # draw the match\n",
    "                ptA = (int(kpsA[queryIdx][0]), int(kpsA[queryIdx][1]))\n",
    "                ptB = (int(kpsB[trainIdx][0]) + wA, int(kpsB[trainIdx][1]))\n",
    "                cv2.line(vis, ptA, ptB, (0, 255, 0), 1)\n",
    " \n",
    "        # return the visualization\n",
    "        return vis\n",
    "    \n",
    "    def trim_image(self, image, trimFrom='RIGHT', trim_threshold=100):\n",
    "        x=0\n",
    "        k=-1\n",
    "        if trimFrom=='RIGHT':\n",
    "            while x ==0:\n",
    "                if k==(-1)*image.shape[1]:\n",
    "                    print(\"no zeroes found\")\n",
    "                    break\n",
    "                print(image[:,k])\n",
    "                if np.count_nonzero(image[:,k])==0:\n",
    "                    k=k-1\n",
    "                else:    \n",
    "                    image=image[:,0:k,:]\n",
    "                    #print(k)\n",
    "                    break\n",
    "            return image\n",
    "        else:\n",
    "            while x ==0:\n",
    "                if k==(-1)*image.shape[0]:\n",
    "                    break\n",
    "                if np.count_nonzero(image[k,:])==0:\n",
    "                    k=k-1\n",
    "                else:    \n",
    "                    image=image[0:k,:,:]\n",
    "                    #print(k)\n",
    "                    break\n",
    "            return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the two images and resize them to have a width of 400 pixels\n",
    "# (for faster processing)\n",
    "\n",
    "\n",
    "\n",
    "imageA = cv2.imread('images/rack2_image1.jpg')\n",
    "imageB = cv2.imread('images/rack2_image2.jpg')\n",
    "imageC = cv2.imread('images/rack2_image3.jpg')\n",
    "imageD = cv2.imread('images/rack2_image4.jpg')\n",
    "imageE = cv2.imread('images/rack2_image5.jpg')\n",
    "imageF = cv2.imread('images/rack2_image6.jpg')\n",
    "imageG = cv2.imread('images/rack2_image7.jpg')\n",
    "imageH = cv2.imread('images/rack2_image8.jpg')\n",
    "imageI = cv2.imread('images/rack_image9.jpg')\n",
    "imageJ = cv2.imread('images/rack_image10.jpg')\n",
    "imageK = cv2.imread('images/rack_image11.jpg')\n",
    "imageL = cv2.imread('images/rack_image12.jpg')\n",
    "imageM = cv2.imread('images/rack_image13.jpg')\n",
    "\n",
    "#result0 = cv2.imread(\"images/rack_result0.jpg\")\n",
    "#imageA = imutils.resize(imageA, width=400)\n",
    "#imageB = imutils.resize(imageB, width=400)\n",
    "#imageC = imutils.resize(imageC, width=400)\n",
    "#imageD = imutils.resize(imageD, width=400)\n",
    "\n",
    "#cv2.imshow(\"Image A\", imageA)\n",
    "#cv2.imshow(\"Image B\", imageB)\n",
    "#cv2.imshow(\"Image C\", imageC)\n",
    "#cv2.imshow(\"Image D\", imageD)\n",
    "#cv2.imshow(\"1\", images[0])\n",
    "#[imageM,imageL,imageK,imageJ,imageI,imageH,imageG,imageF,imageE,imageD,imageC,imageB,imageA]\n",
    "#images = [imageA, imageB, imageC, imageD, imageE, imageF, imageG, imageH]\n",
    "images = [imageA, imageC, imageE]   \n",
    "    # stitch the images together to create a panorama\n",
    "stitcher = Stitcher()\n",
    "result = None\n",
    "for i in range(0, len(images)-1):\n",
    "    if i==0:\n",
    "        result = images[i]\n",
    "\n",
    "    (result, vis) = stitcher.stitchHorizontal([result, images[i+1]], ratio=0.5, reprojThresh=20.0, showMatches=True)\n",
    "#result = imutils.resize(result, height=imageD.shape[1])\n",
    "#cv2.imwrite(\"images/rack2_result0.jpg\",result)\n",
    "#cv2.imwrite(\"images/output.jpg\",result) \n",
    "# show the images\n",
    "\n",
    "    cv2.imshow(\"Keypoint Matches\"+str(i), vis)\n",
    "    cv2.imshow(\"Result\"+str(i), result)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# load the two images and resize them to have a width of 400 pixels\n",
    "# (for faster processing)\n",
    "image1 = 'images/img_hpart1.jpg'\n",
    "image2 = 'images/img_hpart2.jpg'\n",
    "image3 = 'images/img_hpart3.jpg'\n",
    "image4 = 'images/img_hpart4.jpg'\n",
    "imageA = cv2.imread(image1)\n",
    "imageB = cv2.imread(image2)\n",
    "imageC = cv2.imread(image3)\n",
    "imageD = cv2.imread(image4)\n",
    "\n",
    "#imageA = imutils.resize(imageA, width=400)\n",
    "#imageB = imutils.resize(imageB, width=400)\n",
    "#imageC = imutils.resize(imageC, width=400)\n",
    "#imageD = imutils.resize(imageD, width=400)\n",
    "\n",
    "#cv2.imshow(\"Image A\", imageA)\n",
    "#cv2.imshow(\"Image B\", imageB)\n",
    "#cv2.imshow(\"Image C\", imageC)\n",
    "#cv2.imshow(\"Image D\", imageD)\n",
    "\n",
    "\n",
    "# stitch the images together to create a panorama\n",
    "stitcher = Stitcher()\n",
    "(result, vis) = stitcher.stitchHorizontal([imageA, imageB, imageC, imageD], showMatches=True)\n",
    "result = imutils.resize(result, height=imageD.shape[1])\n",
    "#cv2.imwrite(\"images/output.jpg\",result) \n",
    "# show the images\n",
    "\n",
    "#cv2.imshow(\"Keypoint Matches\", vis)\n",
    "cv2.imshow(\"Result\", result)\"\"\"\n",
    "\n",
    "################# VERTICAL TEST\n",
    "\n",
    "image1 = 'images/img_vpart1.jpg'\n",
    "image2 = 'images/img_vpart2.jpg'\n",
    "image3 = 'images/img_vpart3.jpg'\n",
    "image4 = 'images/img_vpart4.jpg'\n",
    "imageA = cv2.imread(image1)\n",
    "imageB = cv2.imread(image2)\n",
    "imageC = cv2.imread(image3)\n",
    "imageD = cv2.imread(image4)\n",
    "\n",
    "#imageA = imutils.resize(imageA, width=400)\n",
    "#imageB = imutils.resize(imageB, width=400)\n",
    "#imageC = imutils.resize(imageC, width=400)\n",
    "#imageD = imutils.resize(imageD, width=400)\n",
    "\n",
    "#cv2.imshow(\"Image A\", imageA)\n",
    "#cv2.imshow(\"Image B\", imageB)\n",
    "#cv2.imshow(\"Image C\", imageC)\n",
    "#cv2.imshow(\"Image D\", imageD)\n",
    "\n",
    "\n",
    "# stitch the images together to create a panorama\n",
    "stitcher = Stitcher()\n",
    "(result, vis) = stitcher.stitchVertical([imageA, imageB, imageC, imageD], showMatches=True)\n",
    "result = imutils.resize(result, width=imageD.shape[0])\n",
    "#cv2.imwrite(\"images/output.jpg\",result) \n",
    "# show the images\n",
    "\n",
    "#cv2.imshow(\"Keypoint Matches\", vis)\n",
    "cv2.imshow(\"Result\", result)\n",
    "\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyallwindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from time import sleep\n",
    "\n",
    "cap = cv2.VideoCapture(\"videos/rack_video1.mp4\")\n",
    "stitcher = Stitcher()\n",
    "result = None\n",
    "counter = 0\n",
    "while True:\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        sleep(5)\n",
    "    ret, image = cap.read()\n",
    "    if ret:\n",
    "        cv2.imshow(\"frame #\"+str(counter), image)\n",
    "        if counter%5 == 0:\n",
    "            #image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            (result, vis) = stitcher.stitchHorizontal([result, image], showMatches=True)\n",
    "            result = imutils.resize(result, height=image.shape[1])\n",
    "        counter+=1\n",
    "    else:\n",
    "        print(\"cannot read video\")\n",
    "        break\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stitcher = Stitcher()\n",
    "result = None\n",
    "counter = 0\n",
    "count=1\n",
    "def process_image(image):\n",
    "    \n",
    "    global counter\n",
    "    global count\n",
    "    global result\n",
    "    global stitcher\n",
    "    #plt.imshow(image)\n",
    "    #cv2.imshow(\"abc\", image)\n",
    "    if counter == 0:\n",
    "        result = image\n",
    "        cv2.imwrite(\"images/rack4_image\"+str(count)+\".jpg\",image)\n",
    "    elif counter%20 == 0:\n",
    "        #image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        #zeroes = np.zeros((15*count,image.shape[1],3), dtype=np.uint8)\n",
    "        #temp_image = cv2.resize(image, (image.shape[1], image.shape[0]-9*count) )\n",
    "        #image = np.concatenate((temp_image, zeroes), axis=0)\n",
    "        (result, vis) = stitcher.stitchHorizontal([result, image], showMatches=True, trim_threshold=40)\n",
    "        \n",
    "        #result = imutils.resize(result, height=image.shape[1])\n",
    "        cv2.imwrite(\"result/rack4_result\"+str(count)+\".jpg\",result)\n",
    "        cv2.imwrite(\"images/rack4_image\"+str(count)+\".jpg\",image)\n",
    "        count+=1\n",
    "    counter+=1\n",
    "    return imutils.resize(result, height=image.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_output = 'videos/rack4_video4_output.mp4'\n",
    "## To speed up the testing process you may want to try your pipeline on a shorter subclip of the video\n",
    "## To do so add .subclip(start_second,end_second) to the end of the line below\n",
    "## Where start_second and end_second are integer values representing the start and end of the subclip\n",
    "## You may also uncomment the following line for a subclip of the first 5 seconds\n",
    "clip1 = VideoFileClip(\"videos/rack4_video4.mp4\")\n",
    "#clip1 = VideoFileClip(\"test_videos/solidWhiteRight.mp4\")\n",
    "white_clip = clip1.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "%time white_clip.write_videofile(white_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 8\n",
    "for i in range(7,0,-1):\n",
    "    image = cv2.imread(\"images/rack4_image\"+str(i)+\".jpg\")\n",
    "    if i==7:\n",
    "        result = image\n",
    "    else:\n",
    "        (result, vis) = stitcher.stitchHorizontal([image, result], showMatches=True)\n",
    "        cv2.imwrite(\"images/rack4_result\"+str(count)+\".jpg\",result)\n",
    "        count+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageA = cv2.imread(\"images/rack4_result7.jpg\")\n",
    "imageB = cv2.imread(\"images/rack4_result13.jpg\")\n",
    "(result, vis) = stitcher.stitchHorizontal([imageA, imageB], showMatches=True)\n",
    "cv2.imwrite(\"images/rack4_result14.jpg\",result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(imageB.shape)\n",
    "plt.imshow(imageB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"images/rack4_image1.jpg\")\n",
    "print(img.shape)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"images/rack4_image1.jpg\")\n",
    "print(image.shape)\n",
    "zeroes = np.zeros((9,image.shape[1],3), dtype=int)\n",
    "print(zeroes.shape)\n",
    "print(zeroes)\n",
    "temp_image = cv2.resize(image, (image.shape[1], image.shape[0]-9) )\n",
    "print(temp_image.shape)\n",
    "print(temp_image)\n",
    "#plt.imshow(temp_image)\n",
    "img = np.concatenate((temp_image, zeroes), axis=0)\n",
    "#zeroes[0:image.shape[0]-9,:] = temp_image[:,:]\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the two images and resize them to have a width of 400 pixels\n",
    "# (for faster processing)\n",
    "\n",
    "\n",
    "\n",
    "imageA = cv2.imread('images/rack_image1.jpg')\n",
    "imageB = cv2.imread('images/rack_image2.jpg')\n",
    "imageC = cv2.imread('images/rack_image3.jpg')\n",
    "imageD = cv2.imread('images/rack_image4.jpg')\n",
    "imageE = cv2.imread('images/rack_image5.jpg')\n",
    "imageF = cv2.imread('images/rack_image6.jpg')\n",
    "imageG = cv2.imread('images/rack_image7.jpg')\n",
    "imageH = cv2.imread('images/rack_image8.jpg')\n",
    "imageI = cv2.imread('images/rack_image9.jpg')\n",
    "imageJ = cv2.imread('images/rack_image10.jpg')\n",
    "imageK = cv2.imread('images/rack_image11.jpg')\n",
    "imageL = cv2.imread('images/rack_image12.jpg')\n",
    "imageM = cv2.imread('images/rack_image13.jpg')\n",
    "\n",
    "#imageA = imutils.resize(imageA, width=400)\n",
    "#imageB = imutils.resize(imageB, width=400)\n",
    "#imageC = imutils.resize(imageC, width=400)\n",
    "#imageD = imutils.resize(imageD, width=400)\n",
    "\n",
    "#cv2.imshow(\"Image A\", imageA)\n",
    "#cv2.imshow(\"Image B\", imageB)\n",
    "#cv2.imshow(\"Image C\", imageC)\n",
    "#cv2.imshow(\"Image D\", imageD)\n",
    "#cv2.imshow(\"1\", images[0])\n",
    "#[imageM,imageL,imageK,imageJ,imageI,imageH,imageG,imageF,imageE,imageD,imageC,imageB,imageA]\n",
    "    \n",
    "    # stitch the images together to create a panorama\n",
    "stitcher = Stitcher()\n",
    "(result, vis) = stitcher.stitchHorizontal([imageD,imageA], showMatches=True)\n",
    "result = imutils.resize(result, height=imageD.shape[1])\n",
    "cv2.imwrite(\"images/rack_result0.jpg\",result)\n",
    "#cv2.imwrite(\"images/output.jpg\",result) \n",
    "# show the images\n",
    "\n",
    "#cv2.imshow(\"Keypoint Matches\", vis)\n",
    "cv2.imshow(\"Result\", result)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:default]",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
